# Sentiment-Analysis-IMDB

In this problem I had to work on IMDB dataset to create an NLP model for sentiment analysis. IMDB dataset contains 50000 movie reviews and their sentiment. The dataset is divided in 50-50% train & test dataset.

First of all to import the dataset I am using tensorflow datasets library, I am importing dataset from the tensorflow datasets as shown below. So, I get original IMDB dataset split in 25000 train set & 25000 test set. I load the data in train data & test data.

In the pre-processing step I have not used any stop words removal or any stemming of the words. The main reason behind not using stop word removal is that If we have a review which says “I told you that movie was not Good” the sentiment of this movie review is Negative, and if we perform stop word removal then we are left with “told movie good”, now originally the sentiment was Negative but “told movie good” looks like positive sentiment. So, sometimes stop words removal can alter the meaning of the sentence very badly, that is why I have not used stop words removal as pre-processing step. I have tried the same model with stop words removal but I got 2-3 percent less accuracy when the stop words were removed compared to model with no stop words removed. I used simple tokenizer with 20000 number of words and oov token of <00V>. So, tokenizer will divide the sentences into smaller parts into words and remove some punctuation. Example “Natural Language Processing” will be divided into [“natural”,”language”,”processing”]. I am using 20000 number of words in the tokenizer which means that this tokenizer will learn/tokenize 20000 new words from the training dataset and assigns sequence of integers to the word (each integer being the index of a token in a directory). Oov token is used when there is a word which is not in the 20000 word dictionary that we just created. After that we use fit on texts method as shown in the second line of the image shown above. Fit on texts will update the internal vocabulary based on a list of texts, in here we assume each entry of list to be a token( each words). 

After that we use word_index method as shown above to obtain the word index of all the words that we have. Now, we will use texts_to_sequences method to transform each sentence of the train dataset to a sequence of integers. In the texts_to_sequences while creating a 20000 word dictionary only top (num_words – 1) unique words will be taken into account. Other words will be assigned oov_token. After obtaining the training sentences as a sequence of integers, we have to make sure that all the sentences are of the same length and for that we are using pad_sequences with max_length parameter as 400, which means if our sentence is more then 400 words long then we will remove the words from the back, and if the sentence is less then 400 words then we will add padding of zeros after the sentence to make all the sentences of the same length. Now, finally our sentences are tokenized, converted into numbers and padded so now let’s create the model. I have experimented with many models in the beginning I started with word embedding layer and after that I had a bidirectional LSTM with 64 units. Using that model I was able to obtain 85 percent accuracy. Then I added another LSTM layer in that model and I got testing accuracy of 86 percent. But, after that If I added another layer I was not able to get improvement in the accuracy as my model started overfitting on the training dataset. After that I tried several different model like model with only embedding layer and Conv-1D layer, then I also tried embedding layer with GRU layer with which I was able to get 87 percent accuracy on the test dataset. Finally, the model shown below was the best model for the IMDB sentiment analysis. In this model I have the word embedding layer as the first layer of the model. The embedding layer turns the positive integers (indexes of words) into dense vectors of fixed size. In short it will create word embedding with 128 embedding dimensions and 20000 words in our dictionary. After that layer I have a single Convolution 1D layer of 128 filters of kernel size =5 and I am using activation function relu. After that layer I have Bidirectional GRU laeyer of 32 units and then I am using fully connected layer with 6 nodes & relu activation function. Then finally at the output layer I have single node with sigmoid activation function.

Here is the model summary of the above model. According to my research the hybrid model like the one shown above performs much better compared to the model which only contains one type like only LSTM or only GRU. The combination of Convolution layer, GRU layer and fully connected layer was the best.

Above you will find the code. Enjy :)
